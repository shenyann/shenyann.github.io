<!DOCTYPE HTML>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <title>{{ site.name }}</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="{{ site.name }}" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="{{ site.baseurl }}/style.css" />
  <link rel="canonical" href="{{ page.url | replace:'index.html','' | prepend: site.baseurl | prepend: site.url }}">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                  <name> {{ site.name }} </name>
              </h1>
              <p align="center">
                  <email> shenyan at google dot com </email>
              </p>
              <p>
I am a Research Scientist at <a href="https://research.google/teams/perception/">Google Research, Perception</a>, where I work on video-text modeling and its applications. </p>

<p>
I did my PhD at the <a href="https://cse.msu.edu/">Computer Science Department</a> at <a href="https://msu.edu/">Michigan State University</a>, working with Professor <a href="https://www.egr.msu.edu/~mizhang/">Mi Zhang</a>.
</p> 

<p>
During my PhD, I was fortunate to work with people at Bosch Research, Bytedance AML, Abacus.AI, Argo AI, and Google Research (Perception and Brain team).
</p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=-shYRd8AAAAJ&hl=en">Google Scholar</a> /
                <a href="{{ site.user.linkedin | prepend: 'https://www.linkedin.com/in/' }}">LinkedIn</a> /
                <a href="https://twitter.com/shenyan82">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="{{ site.user.photo }}">
            </td>
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Research</h2>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/videoprism.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2402.13217">
                  <papertitle>VideoPrism: A Foundational Visual Encoder for Video Understanding</papertitle>
                </a>
                <br>
                <a href="https://research.google/research-areas/machine-perception/">Google Research, VFFM</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html">blog</a> /
                <a href="https://arxiv.org/abs/2402.13217">arXiv</a> /
                <a href="data/zhao2024videoprism.bib">bibtex</a>
                <p></p>
                <p>A general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/palm2-vadaptor.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.09237">
                  <papertitle>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=rv-aTqkAAAAJ&hl=en">Junfei Xiao</a>,
                <a href="https://scholar.google.com/citations?user=iTdxOZ8AAAAJ&hl=en">Zheng Xu</a>,
                <a href="https://scholar.google.com/citations?user=FJ-huxgAAAAJ&hl=en">Alan Yuille</a>,
                <strong>Shen Yan</strong>*,
                <a href="https://scholar.google.com/citations?user=OKUiJEwAAAAJ&hl=en">Boyu Wang*</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2402.10896">arXiv</a> /
                <a href="data/xu2023pixelllm.bib">bibtex</a>
                <p></p>
                <p>We employ a progressively aligned tiny PaLM-2 as the vision-language adaptor.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/pixellm.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.09237">
                  <papertitle>Pixel Aligned Language Models</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=2GKLw94AAAAJ&hl=en">Jiarui Xu</a>,
                <a href="https://scholar.google.com/citations?user=47n-0mwAAAAJ&hl=en">Xingyi Zhou</a>,
                <strong>Shen Yan</strong>,
                <a href="https://scholar.google.co.uk/citations?user=qCrypnoAAAAJ&hl=en">Xiuye Gu</a>,
                <a href="https://scholar.google.co.uk/citations?user=l2FS2_IAAAAJ&hl=en">Anurag Arnab</a>,
                <a href="https://scholar.google.com/citations?user=vQa7heEAAAAJ&hl=en">Chen Sun</a>,
                <a href="https://scholar.google.com/citations?user=Y8O9N_0AAAAJ&hl=en">Xiaolong Wang</a>,
                <a href="https://scholar.google.com/citations?user=IvqCXP4AAAAJ&hl=en">Cordelia Schmid</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2312.09237">arXiv</a> /
                <a href="data/xu2023pixelllm.bib">bibtex</a>
                <p></p>
                <p>We propose PixelLLM to equip LLMs with pixel-aligned localization capability.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/unloc.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2308.11062">
                  <papertitle>UnLoc: A Unified Framework for Video Localization Tasks</papertitle>
                </a>
                <br>
                <strong>Shen Yan</strong>*,
                <a href="https://scholar.google.com/citations?user=vM1SktEAAAAJ&hl=en">Xuehan Xiong*</a>,
                <a href="https://scholar.google.co.uk/citations?user=-_2vpWwAAAAJ&hl=en">Arsha Nagrani</a>,
                <a href="https://scholar.google.co.uk/citations?user=l2FS2_IAAAAJ&hl=en">Anurag Arnab</a>,
                <a href="https://scholar.google.com/citations?user=opL6CL8AAAAJ&hl=en">Zhonghao Wang</a>,
                <a href="https://dblp.org/pid/50/5613.html">Weina Ge</a>,
                <a href="https://scholar.google.com/citations?user=RqOzJR0AAAAJ&hl=en">David Ross</a>,
                <a href="https://scholar.google.com/citations?user=IvqCXP4AAAAJ&hl=en">Cordelia Schmid</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2308.11062">arXiv</a> /
                <a href="data/yan2023unloc.bib">bibtex</a>
                <p></p>
                <p>UnLoc unifies moment retrieval, temporal localization and action segmentation with a single stage model.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/output-vcoca.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2212.04979">
                  <papertitle>VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners</papertitle>
                </a>
                <br>
                <strong>Shen Yan</strong>*,
                <a href="https://www.linkedin.com/in/taozhu99/">Tao Zhu*</a>,
                <a href="https://scholar.google.com/citations?user=GgD-B68AAAAJ&hl=en">Zirui Wang</a>,
                <a href="https://scholar.google.com/citations?user=Q82vvqcAAAAJ&hl=en">Yuan Cao</a>,
                <a href="https://scholar.google.com/citations?user=r3A90uAAAAAJ&hl=en">Mi Zhang</a>,
                <a href="https://scholar.google.com/citations?user=QkZ8ZHEAAAAJ&hl=en">Soham Ghosh</a>,
                <a href="https://scholar.google.com/citations?user=55FnA9wAAAAJ&hl=en">Yonghui Wu</a>,
                <a href="https://scholar.google.com/citations?user=-CLCMk4AAAAJ&hl=en">Jiahui Yu</a>
                <br>
                <em>arXiv</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2212.04979">arXiv</a> /
                <a href="data/yan2022videococa.bib">bibtex</a>
                <p></p>
                <p>VideoCoCa maximally reuses pretrained CoCa and minimizes additional training cost.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/output-softcrop.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2211.04625">
                  <papertitle>Soft Augmentation for Image Classification</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=nVWQwHkAAAAJ&hl=en">Yang Liu</a>,
                <strong>Shen Yan</strong>,
                <a href="https://scholar.google.com/citations?user=tT2TC-UAAAAJ&hl=en">Laura Leal-Taix√©</a>,
                <a href="https://scholar.google.com/citations?user=vjZrDKQAAAAJ&hl=en">James Hays</a>,
                <a href="https://scholar.google.com/citations?user=9B8PoXUAAAAJ&hl=en">Deva Ramanan</a>
                <br>
                <em>CVPR</em>, 2023 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2211.04625">arXiv</a> /
                <a href= "data/liu2022soft.bib">bibtex</a>
                <p></p>
                <p>Soft augmentations produce better calibrated models on occluded examples.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/mtv_img.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2201.04288">
                  <papertitle>Multiview Transformers for Video Recognition</papertitle>
                </a>
                <br>
                <strong>Shen Yan</strong>,
                <a href="https://scholar.google.com/citations?user=vM1SktEAAAAJ&hl=en">Xuehan Xiong</a>,
                <a href="https://scholar.google.co.uk/citations?user=l2FS2_IAAAAJ&hl=en">Anurag Arnab</a>,
                <a href="https://scholar.google.com/citations?user=pZJV-Z0AAAAJ&hl=en">Zhichao Lu</a>,
                <a href="https://scholar.google.com/citations?user=r3A90uAAAAAJ&hl=en">Mi Zhang</a>,
                <a href="https://scholar.google.com/citations?user=vQa7heEAAAAJ&hl=en">Chen Sun</a>,
                <a href="https://scholar.google.com/citations?user=IvqCXP4AAAAJ&hl=en">Cordelia Schmid</a>
                <br>
                <em>CVPR</em>, 2022 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2201.04288">arXiv</a> /
                <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/mtv">code</a> /
                <a href="data/yan2022mtv.bib">bibtex</a>
                <p></p>
                <p>A simple method for capturing multiresolution temporal context in transformers.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/deepaa.png' width="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.06172">
                  <papertitle>Deep AutoAugment</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=6GyET_8AAAAJ&hl=en">Yu Zheng</a>,
                <a href="https://scholar.google.com/citations?user=nZr0oXQAAAAJ&hl=en">Zhi Zhang</a>,
                <strong>Shen Yan</strong>,
                <a href="https://scholar.google.com/citations?user=r3A90uAAAAAJ&hl=en">Mi Zhang</a>
                <br>
                <em>ICLR</em>, 2022 &nbsp
                <br>
                <a href="https://arxiv.org/abs/2203.06172">arXiv</a> /
                <a href="https://github.com/MSU-MLSys-Lab/DeepAA">code</a> /
                <a href="data/zheng2022deep.bib">bibtex</a> /
                <a href="data/2022_DeepAA_Slides.pdf">slides</a>
                <p></p>
                <p>Build a data augmentation policy progressively based on regularized gradient matching.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/x11_surrogate.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.03602">
                <papertitle>NAS-Bench-x11 and the Power of Learning Curves</papertitle>
              </a>
              <br>
              <strong>Shen Yan</strong>*,
              <a href="https://scholar.google.com/citations?user=LS6HY-gAAAAJ&hl=en">Colin White*</a>,
              <a href="https://scholar.google.com/citations?user=WtCWzFwAAAAJ&hl=en">Yash Savani</a>,
              <a href="https://scholar.google.com/citations?user=YUrxwrkAAAAJ&hl=en">Frank Hutter</a>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp
              <br>
              <a href="https://arxiv.org/abs/2111.03602">arXiv</a> /
              <a href="https://github.com/automl/nas-bench-x11">code</a> /
              <a href="data/yan2021x11.bib">bibtex</a> /
              <a href="data/nbx11_slides.pdf">slides</a>
              <p></p>
              <p>A surrogate method to create multi-fidelity NAS benchmarks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/cate.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2102.07108">
                <papertitle>CATE: Computation-aware Neural Architecture Encoding with Transformers</papertitle>
              </a>
              <br>
              <strong>Shen Yan</strong>,
              <a href="https://scholar.google.com/citations?user=PHoJwakAAAAJ&hl=en">Kaiqiang Song</a>,
              <a href="https://scholar.google.com/citations?user=22ohn6AAAAAJ&hl=en">Fei Liu</a>,
              <a href="https://scholar.google.com/citations?user=r3A90uAAAAAJ&hl=en">Mi Zhang</a>
              <br>
              <em>ICML</em>, 2021 <font color="red"><strong>(Long Presentation)</strong></font>
              <br>
              video: <a href="https://crossminds.ai/video/cate-computation-aware-neural-architecture-encoding-with-transformers-614bc93a3c7a224a9090282f/">17 min</a>/
              <a href="https://arxiv.org/abs/2102.07108">arXiv</a> /
              <a href="https://github.com/MSU-MLSys-Lab/CATE">code</a> /
              <a href="data/yan2021cate.bib">bibtex</a>
              <p></p>
              <p>Pre-training computation-aware architecture embeddings can also help with architecture search.</p>
            </td>
          </tr>

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/unsup.png' width="160"></div>
                <img src='images/sup.png' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.06936">
                <papertitle>Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?</papertitle>
              </a>
              <br>
              <strong>Shen Yan</strong>,
              <a href="https://scholar.google.com/citations?user=6GyET_8AAAAJ&hl=en">Yu Zheng</a>,
              <a href="https://wei-ao.github.io/">Wei Ao</a>,
              <a href="https://scholar.google.com/citations?user=fgF1HTsAAAAJ&hl=en">Xiao Zeng</a>,
              <a href="https://www.egr.msu.edu/~mizhang/">Mi Zhang</a>
              <br>
              <em>NeurIPS</em>, 2020 &nbsp
              <br>
              video: <a href="https://crossminds.ai/video/does-unsupervised-architecture-representation-learning-help-neural-architecture-search-5fb82261890833803bc7e7ed/">3 min</a>/
              <a href="https://arxiv.org/abs/2006.06936">arXiv</a> /
              <a href="https://github.com/MSU-MLSys-Lab/arch2vec">code</a> /
              <a href="data/yan2020arch.bib">bibtex</a>
              <p></p>
              <p>Pre-training structure-aware architecture embeddings help  architecture search.</p>
            </td>
          </tr>


          <tr onmouseout="mutual_stop()" onmouseover="mutual_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mutual_image'>
                  <img src='images/mutual_later.png' width="160"></div>
                <img src='images/mutualnet.png' width="160">
              </div>
              <script type="text/javascript">
                function mutual_start() {
                  document.getElementById('mutual_image').style.opacity = "1";
                }

                function mutual_stop() {
                  document.getElementById('mutual_image').style.opacity = "0";
                }
                mutual_stop()
              </script>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460290.pdf">
                <papertitle>MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/taojiannanyang/home">Taojiannan Yang</a>,
              <a href="https://scholar.google.com/citations?user=8aO4k80AAAAJ&hl=en">Sijie Zhu</a>,
              <a href="https://webpages.uncc.edu/cchen62/">Chen Chen</a>,
              <strong>Shen Yan</strong>,
              <a href="https://www.egr.msu.edu/~mizhang/">Mi Zhang</a>,
              <a href="https://scholar.google.com/citations?user=i69G8doAAAAJ&hl=en">Andrew Wills</a>
              <br>
              <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
              video: <a href="https://www.youtube.com/watch?v=RzzxhkJConk">10 min</a>/
              <a href="https://arxiv.org/abs/1909.12978">arXiv</a> /
              <a href="https://github.com/taoyang1122/MutualNet">code</a> /
              <a href="data/yang2020mutual.bib">bibtex</a>
              <p></p>
              <p>Mutual learning with input resolution and network width improves accuracy-efficiency tradeoffs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/mixup.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2001.00677">
                <papertitle>Improve Unsupervised Domain Adaptation with Mixup Training</papertitle>
              </a>
              <br>
              <strong>Shen Yan</strong>,
              <a href="https://scholar.google.com/citations?user=9snl7bcAAAAJ&hl=en">Huan Song</a>,
              <a href="https://scholar.google.com/citations?user=GaPN-R4AAAAJ&hl=en">Nanxiang Li</a>,
              <a href="https://scholar.google.com/citations?user=X4dfV7IAAAAJ&hl=en">Lincan Zou</a>,
              <a href="https://sites.google.com/site/liurenshomepage/">Liu Ren</a>
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2001.00677">arXiv</a> /
              <a href="https://github.com/facebookresearch/DomainBed">code</a> /
              <a href="data/yan2020mix.bib">bibtex</a>
              <p></p>
              <p>Mixup can also help with unsupervised domain adaptation.</p>
            </td>
          </tr>

          <tr onmouseout="dff_stop()" onmouseover="dff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dff_image'>
                  <img src='images/dff_resize.png' width="160"></div>
                <img src='images/ce_resize.png' width="160">
              </div>
              <script type="text/javascript">
                function dff_start() {
                  document.getElementById('dff_image').style.opacity = "1";
                }

                function dff_stop() {
                  document.getElementById('dff_image').style.opacity = "0";
                }
                dff_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.bmva.org/bmvc/2017/papers/paper165/paper165.pdf">
                <papertitle>Deep Fisher Faces</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=Y9zbHswAAAAJ&hl=en">Harald Hanselmann</a>,
              <strong>Shen Yan</strong>,
              <a href="https://scholar.google.de/citations?user=6C8rf-0AAAAJ&hl=de">Hermann Ney</a>
              <br>
              <em>BMVC</em>, 2017
              <br>
              <a href="data/hanselmann2017dff.bib">bibtex</a>
              <p></p>
              <p>We extend the center loss with an inter-class loss reminiscent of the popular early face recognition approach Fisherfaces.</p>
            </td>
          </tr>
            </tbody></table>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h2>Service</h2>
            </td>
          </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="5"></tbody>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle"><img src="images/ml_logo.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://sites.google.com/view/automl2021">PC member, AutoML Workshop, ICML 2021</a>
              <br>
              <br>
              <a href="https://sites.google.com/view/nas2021/organization?authuser=0">PC member, NAS Workshop, ICLR 2021</a>
              <br>
              <br>
              <a href="https://icml.cc/Conferences/2020/Reviewers">Reviewer, ICML 2020, 2021, 2022, 2023</a>
              <br>
              <br>
              <a href="https://iclr.cc/Conferences/2021/Reviewers">Reviewer, ICLR 2021, 2022, 2023</a>
              <br>
              <br>
              <a href="https://neurips.cc/Conferences/2020/ProgramCommittee">Reviewer, NeurIPS 2020, 2021, 2022, 2023</a>
              <br>
              <br>
              <a href="http://cvpr2021.thecvf.com/node/181">Reviewer, CVPR 2021, 2022, 2023</a>
              <br>
              <br>
              <a href="http://iccv2021.thecvf.com/node/39">Reviewer, ICCV 2021, 2023</a>
              <br>
              <br>
              <a href="https://eccv2022.ecva.net/">Reviewer, ECCV 2022</a>
              <br>
              <br>
              <a href="https://www.jmlr.org/tmlr/">Reviewer, TMLR 2022, 2023</a>
              <br>
              <br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">Reviewer, PAMI 2022, 2023</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kinect.png" width="160">
            </td>
            <td width="75%" valign="center">
              <a href="https://www.lfb.rwth-aachen.de/en/education/ip/">TA for Bachelor, Kinect Programming, Fall 2015</a>
            </td>
          </tr>
          </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a style="font-size:small;" href="https://jonbarron.info">This guy makes a nice webpage.</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

